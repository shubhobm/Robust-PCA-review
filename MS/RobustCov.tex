\section{Robust covariance estimation, data transformation, and beyond}
\label{Section:sec2}

\subsection{Robust covariance matrices, projection pursuit}

The earliest approaches to robust PCA were based on robustly estimating the population covariance matrix, and using eigenvectors of that estimate as principal components. Some methods of robust covariance matrix estimation include the Minimum Volume Ellipsoid estimator \citep{Rousseeuw84}, a projection-based estimator by \citep{maronna76}, the Minimum Covariance Determinant (MCD) estimator \citep{rousseeuw85} and the Stael-Donoho estimators \citep{MaronnaYohai95,ZuoCui05}. Although these estimators have high breakdown points, they suffered from two severe drawbacks. Firstly the explicit evaluation of the population covariance matrix meant that obtaining principal components were not possible when $n < p$. Secondly, even when $n > p$, these methods become computationally intensive with large data dimensions.

\subsection{Data transformation}

\subsection{Robust PCA and outlier detection}
Aside from obtaining a lower dimensional projection of the data matrix $\bfX$ in spite of outliers that is close enough to the projection of $\bfX$ by the first few population eigenvectors, detecting the outliers themselves is also closely associated with robust PCA. These samples can be of interest for mechanistic reasons. For example in the analysis of near infra-red absorbance for 39 gasoline samples over 226 wavelengths using ROBPCA \citep{hubert05}, six compounds are flagged as outliers, and these turn out to be the only samples containing alcohol. \cite{hubert05} also introduced a notion of outlier diagnostics that is applicable to any method of robust PCA and can serve as a means to compare different relevant techniques as well.

We illustrate this in \ref{fig:figROBPCA}. Here we consider data in 3 dimensions, and consider the relative position of the samples with respect to the two-dimensional principal component subspace $\cM$. We can classify such points into four categories:

\begin{enumerate}
\item{\it Regular observations}: points that form a homogeneous group close to $\cM$ ($A$ and $B$ in figure);
\item{\it Good leverage points}: points that lie close to $\cM$, but at a distance from the regular observations ($C$ in figure);
\item{\it Orthogonal outliers}: These points (point $D$ in figure) lie far away from their projections on $\cM$ (point $D'$, but the projections themselves are close to the regular observations;
\item{\it Bad leverage points}: These points are also far away from their projections on $\cM$ ($E$ and $E'$ respectively), but the projections are also far away from the regular observations.
\end{enumerate}

\cite{hubert05} introduced the concept of \textit{score distance} (SD) and \textit{orthogonal distance} (OD) to distinguish between these four types of points. With our notation, for the $i^\text{th}$ observation these distances are defined as:
%
$$
SD_i = \sum_{j=1}^q \frac{t_{ij}}{\lambda_j};\
\quad OD_i  =\| ( \bfI - \bfW_k \bfW_k^T) (\bfx_i - \bfmu) \|
$$
%
The SD can be interpreted as the weighted distance of the projection of a point on the hyperplane formed by the first $k$ PCs, while OD is the orthogonal distance of that point and the $k$-PC hyperplane. It is now clear from our picture that regular observations have low values of both SD and OD, while bad leverage points have high values of both. An orthogonal outlier has small SD but large OD, whereas a good leverage point has high SD but small OD. To explicitly classify sample points into these 4 categories, \cite{hubert05} use $\sqrt {\chi^2_{k,0.975}}$ and $[ \hat \mu (OD^{2/3}) + \hat \sigma (OD^{2/3}) \Phi^{-1} (0.975) ]^{3/2}$ as upper cutoffs for score distance and orthogonal distance, respectively. Here $\hat \mu$ and $\hat \sigma$ are univariate MCD estimators, and $\Phi$ is the standard normal cumulative distribution function.