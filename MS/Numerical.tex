\section*{\sffamily \Large NUMERICAL EXAMPLES}
\label{section:sec4}

In this section we present two data analytical scenarios to demonstrate the comparative performance and applicability of the different approaches of robust PCA we have discussed.

\subsection*{\sffamily \large Bus data}
Available in the R package \texttt{rrcov}, this dataset consists of the measurements of 18 image features for 218 buses. Following a similar analysis in \cite{maronna06}, pp. 213, we set aside variable 9 and scale the other variables by dividing with their respective median absolute deviations (MAD). We do this done because all the variables had much larger standard deviations compared to their MADs, and variable 9 had MAD = 0. Following this, we compare the performances of the classical PCA (CPCA), PCA based on the eigenvector estimate from the MCD covariance matrix (MPCA), spatial sign-based PCA (SPCA), depth-based weighted sign PCA (DPCA), ROBPCA, and PCP.
We use projection depth as our choice of depth function while doing DPCA.

For all classical PCA methods, we set the number of PCs at 3. We compare the above methods using the distance of actual data and their projections on the principal component space. For PCP, these are is simply row norms of the sparse matrix $\bfS_0$ obtained from the procedure, while for other methods this is the orthogonal distances of corresponding samples. Each of its column lists different quantiles of the squared orthogonal distance for a sample point from the hyperplane formed by top 3 PCs estimated by the corresponding method. Table \ref{table:bus_table2} presents the different quantiles of squares of these distances for all the methods. For DPCA, the estimated principal component subspaces are closer to the data than CPCA for more than 90\% of samples, and the distance only becomes larger for higher quantiles. This means that for CPCA, estimated basis vectors of the hyperspace get pulled by extreme outlying points, while the influence of these outliers is very low for DPCA. SPCA and ROBPCA perform very closely in this respect, the percentage of points that have less squared distance than CPCA being between 80\% and 90\% for both of them. This percentage is only 60\% for PCP and 50\% for MPCA, which suggests that the corresponding 3-dimensional subspace estimated by MCD is possibly not an accurate representation of the truth, and there is probably enough noise in the data apart from the low-rank and sparse components estimated by PCP.

%\begin{table}[t]
%\centering
%    \begin{tabular}{c|ccccc}
%    \hline
%    \begin{large} $q$ \end{large} & \multicolumn{5}{c}{Method of PCA}         \\ \cline{2-6}
%    ~                   & CPCA     & SPCA & ROBPCA & MPCA & DPCA \\\hline 
%    1                   & 0.188         & 0.549     & 0.410  & 0.514     & 0.662     \\
%    2                   & 0.084         & 0.272     & 0.214  & 0.337     & 0.359     \\
%    3                   & 0.044         & 0.182     & 0.121  & 0.227     & 0.237     \\
%    4                   & 0.026         & 0.135     & 0.083  & 0.154     & 0.173     \\
%    5                   & 0.018         & 0.099     & 0.054  & 0.098     & 0.115     \\
%    6                   & 0.012         & 0.069     & 0.036  & 0.070     & 0.084     \\ \hline
%    \end{tabular}
%    \caption{Unexplained proportions of variability by PCA models with $q$ components for bus data}
%    \label{table:bus_table1}
%\end{table}

\begin{table}[t]
\centering
    \begin{tabular}{c|cccccc}
    \hline
    Quantile & \multicolumn{6}{c}{Method of PCA}         \\ \cline{2-7}
    ~                   & CPCA     & SPCA & ROBPCA & MPCA & DPCA & PCP\\\hline 
    10\%      & 1.9       & 1.2       & 1.2    & 1.0       & 1.2       & 1.3\\
    20\%      & 2.3       & 1.6       & 1.6    & 1.3       & 1.6       & 1.8\\
    30\%      & 2.8       & 1.8       & 1.8    & 1.7       & 1.9       & 2.1\\
    40\%      & 3.2       & 2.2       & 2.1    & 2.1       & 2.3       & 2.5\\
    50\%      & 3.7       & 2.6       & 2.5    & 3.1       & 2.6       & 3.2\\
    60\%      & 4.4       & 3.1       & 3.0    & 5.9       & 3.2       & 3.8\\
    70\%      & 5.4       & 3.8       & 3.9    & 25.1      & 3.9       & 5.7\\
    80\%      & 6.5       & 5.2       & 4.8    & 86.1      & 4.8       & 11.9\\
    90\%      & 8.2       & 9.0       & 10.9   & 298.2     & 6.9      & 80.2\\
    Max       & 24        & 1037      & 1055   & 1037      & 980      & 1157\\\hline
    \end{tabular}
    \caption{Quantiles of squared data-to-projection distances for bus data}
    \label{table:bus_table2}
\end{table}

\subsection*{\sffamily \large Image denoising}

\begin{figure}[t]
\centering
\includegraphics[width=.8\textwidth]{all_denoise}
\caption{Denoising results of four images. Left to right in a row indicates the original image, image with noise added, and denoising by DPCA, ROBPCA and PCP, respectively.}
\label{fig:figDenoise}
\end{figure}
