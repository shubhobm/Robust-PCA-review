\section*{\sffamily \Large ROBUST PCA IN OTHER SPACES}
\label{section:Others}

\subsection*{\sffamily \large Kernel PCA}
Kernel PCA is prominent among nonlinear methods of dimension reduction, i.e. methods that aim to reduce the dimension of a nonlinear transformation of the data matrix $\bfX$, instead of a linear mapping of it. This is because even though kernel PCA performs linear PCA on the transformed feature space, it is able to do so without using the transformed random variables. Specifically, suppose the transformation map is $\phi: \BR^p \mapsto \cH$, a reproducing kernel Hilbert space (RKHS). Then there exists a {\it kernel function} $K: \BR^p \times \BR^p \mapsto \BR$ that gives the inner product in the transformed space:
%
\begin{align}\label{eqn:kernelEqn}
K( \bfa, \bfb) = \langle \phi(\bfa), \phi(\bfb) \rangle
\end{align}
%
This `kernel trick' enables us to perform PCA on the transformed data $\{ \phi(\bfx_1), \ldots, \phi(\bfx_n) \}$ just by replacing inner products with the kernel function in (\ref{eqn:kernelEqn}) \citep{Scholkopf99}. As a consequence, it is possible to extend methods of linear PCA to their kernel versions. This holds for robust methods as well. Indeed, \cite{DeBruyneVerdonck10,DebruyneEtal10} provided algorithms that extend SPCA, projection pursuit and ROBPCA into kernel spaces, and \cite{Yangetal14,HuangEtal09} and \cite{HuangReh11} used robust loss functions in the kernel version of a reformulation of the classical PCA problem in (\ref{eqn:eqnPCA1}) and (\ref{eqn:eqnPCAk}). \cite{WangEtal07, DengEtal07} and \cite{PangEtal10} proposed more methods that connect robust classical PCA methods to kernel PCA.

Compared to robust linear PCA, robust kernel PCA has an additional step of reverse mapping the principal component projections on the feature space $\cH$ to the input space $\BR^p$. This is important because the areas of application for robust kernel PCA are concerned with the recognition of shapes that are mostly nonlinear in presense of additional noise: for example in computer vision \citep{Lampart08}, environmental science \citep{HsiehBook}, and neuroimaging \citep{MwangiEtal14}. An exact pre-image, however, may not always exist \citep{Mikaetal99}, and obtaining an approximate reconstruction is challenging because $\cH$ can be infinite dimensional. \cite{Mikaetal99} first provided a solution of this problem using an iterative algorithm. Later on, \cite{KwokTsang03} provided another method of pre-image reconstruction using multidimensional scaling that results in better recovery in the original data space but does not suffer from the instability issues of \cite{Mikaetal99}.

\subsection*{\sffamily \large Functional PCA}
For functional data, the matrix $\bfX$ shall correspond to the realizations of a random function, say $f_X$, that takes values in $L^2 (\cI)$: $\cI$ being a real interval. Following the definition of inner products in the functional space: $ \langle a, b \rangle = \int_\cI a(y) b(y) dy$ for $a,b \in L^2 (\cI)$, one can replace the dot products in (\ref{eqn:eqnPCA1}) and (\ref{eqn:eqnPCAk}) by these inner products to define functional principal components. Analogous to the real setting, the covariance {\it operator} of $f_X$ is defined as $\gamma_X = (f_X - \BE f_X) \otimes (f_X - \BE f_X); a \otimes b: \cH \rightarrow \cH$ so that $(a \otimes b)c = \langle b,c \rangle a$. Following this, $f_X$ has the Karhunen-Lo\'{e}ven expansion:
%
\begin{align}\label{eqn:KL-expansion}
f_X = \BE f_X + \sum_{l=1}^\infty \lambda_l^{1/2} c_l \phi_l
\end{align}
%
where $\{ \lambda_l, l \geq 1, \lambda_l \geq \lambda_{l+1} \}$ and $\{ \phi_l, l \geq 1\}$ are eigenvalues and orthonormal eigenfunctions of $\gamma_X$, respectively, and $c_l = \lambda_l^{-1/2}  \langle f_X - \BE f_X , \phi_l\rangle$ are real-valued random variables. The top eigenfunctions are able to provide a finite dimensional approximation of $f_X$, and turn out to be its principal components.

It is possible to reduce the robust functional PCA problem to robust PCA on real domain by mapping the original data onto a finite set of orthogonal basis functions and working on the matrix of the corresponding coefficients. This approach was taken by \cite{LocantoreEtal99} and \cite{BoenteBarrera15}. However, the smoothing approximations can produce bias that can be avoided using a fully functional approach \citep{ZhangChen07}. Similar to PCA on the real domain, this can be done by robustly estimating $\gamma_X$ or another covariance operator, or directly estimating the top eigenfunctions in \eqref{eqn:KL-expansion} in a robust manner. \cite{Gervini08} took the first approach by formulating a functional version of SPCA, while the work of \cite{BaliEtal11} that performs projection pursuit in the functional domain is prominent in the second category. Theoretical details of these methods, functional outlier detection methods, as well as their comparative performance, can be found in \cite{BaliBoenteReview}.

%There are two kinds of outliers in functional data: an outlier can either be an entire atypical curve that does not conform to the general shape of other functional curves in the data, or it can be an isolated point or group of points in an otherwise typical curve.
