\section*{\sffamily \Large PRINCIPAL COMPONENT PURSUIT}
\label{section:sec3}

The above notion of outliers depends on the fact that the $n \times p$ data matrix $\bfX$ is composed of observations from several independent samples in its rows, and some of these samples have corrupted observations. However, in many practical situations, rows of $\bfX$ might not be independent, the corrupted observations can have a pattern across samples, or both. For example in face or handwriting recognition, each individual picture can be taken as a data matrix. The value of a pixel takes corresponds to an entry in the data matrix, with noisy pixels denoting corrupted measurements. Although the underlying low-rank structure is still of interest in such situations, for example the face of a person or a handwritten digit, this problem is fundamentally different because of the inherent structure present in the data.

\cite{CandesEtal09} first introduced {\it Principal Component Pursuit} (PCP), which decomposes the data matrix into low-rank and sparse components to tackle the above situation. Formally, PCP considers the following additive model:
%
\begin{align}\label{eqn:PCPmodel}
\bfX = \bfL_0 + \bfS_0
\end{align}
%
with rank$(\bfL_0) = r < p$ and $\bfS_0$ sparse. The low-rank and sparse structures are recovered using nuclear norm penalization on the first component and $\ell_1$-norm penalization on the second component, respectively:
%
\begin{align}\label{eqn:PCPobj}
& \text{minimize } \| \bfL \|_* + \| \bfS \|_1; \quad \text{subject to } \bfL + \bfS = \bfX
\end{align}
%
where $\|.\|_*$ denotes the nuclear norm of a matrix, i.e. sum of its singular values, and $\|.\|_1$ denotes $\ell_1$-norm, i.e. sum of the absolute values of its entries. \cite{CandesEtal09} proved that given the true underlying structure is indeed low-rank-plus-sparse, i.e. adheres to the decomposition in (\ref{eqn:PCPmodel}), a polynomial time algorithm based on convex programming can exactly recover these matrices, and this is possible for arbitrary magnitudes of entries in the sparse component.

\subsection*{\sffamily \large PCP and matrix completion}
Both the polynomial time algorithm and arbitrary magnitude of corrupted entries are strengths of PCP over traditional methods of robust PCA. Another reason the PCP is attractive by itself is because with slight modifications, it can perform robust matrix completion. The matrix completion problem attempts to fill in a data matrix $\bfY$ through nuclear norm minimization when only a subset $\Omega \subset \{ 1, \ldots, n\} \times \{ 1, \ldots, p\}$ of its entries are observed. Formally stated, the problem amounts to
%
$$
\text{minimize } \| \bfL \|_*; \quad \text{ subject to } \bfP_\Omega \bfL = \bfY
$$
%
where $\bfP_\Omega$ is the known indicator matrix of non-missing entries: $(\bfP_\Omega)_{ij} = \BI_{(i,j) \in \Omega}$. PCP simply assumes there is a sparse noise component in the incomplete data: $\bfY = \bfP_\Omega (\bfL_0 + \bfS_0)$, and recovers the low-rank structure:
%
\begin{align}\label{eqn:PCPMCobj}
& \text{minimize } \| \bfL \|_* + \| \bfS \|_1; \quad \text{subject to } \bfP_\Omega(\bfL + \bfS) = \bfY
\end{align}
%

\cite{CandesEtal09} showed that it is possible to solve this problem with minimal modifications to their original PCP algorithm that solves (\ref{eqn:PCPobj}). Multiple further studies provided improvements for several aspects of this basic setup. The work of \cite{ChenEtal11} is prominent among them. In particular, they assumed the presence of both errors and missing entries, with deterministic or random support for each of them, and provided theoretical performance guarantees when the fraction of observed entries vanishes as $n \rightarrow \infty$. They also performed worst-case analysis for the errors-only or missing-only scenarios.

\subsection*{\sffamily \large Modifications}
In a subsequent paper, \cite{ZhouEtal10} added an entrywise noise component $\bfZ$ to the objective functions in (\ref{eqn:PCPobj}):
%
\begin{align}\label{eqn:PCPobjZ}
& \text{minimize } \| \bfL \|_* + \| \bfS \|_1; \quad \text{subject to } \bfL + \bfZ + \bfS = \bfX
\end{align}
%
and (\ref{eqn:PCPMCobj}):
%
\begin{align}\label{eqn:PCPMCobjZ}
& \text{minimize } \| \bfL \|_* + \| \bfS \|_1; \quad \text{subject to } \bfP_\Omega(\bfL + \bfZ + \bfS) = \bfY
\end{align}
%
This brought the PCP formulation closer to the classical robust PCA setup that separates a lower-dimensional component in presence of both data-wide additional noise and corrupted entries, with the advantage that here the magnitude and structure of corrupted entries can be arbitrary. Further modifications of PCP include the case when the lower-dimensional component is a union of multiple lower dimensional subspaces \citep{WohlbergEtal12}, adding an $\ell_1/\ell_2$-penalization term on $\bfL$ \citep{TangNehorai11}, a dual formulation of the problem \citep{BeckerEtal11}, and non-convex robust matrix completion \citep{ShangEtal14}. PCP has been an active area of research in the signal and image processing community for the past few years. Please refer to \cite{Bouwmans14} for a detailed review on more modifications of the PCP, algorithmic developments, and its applications in video surveillance.