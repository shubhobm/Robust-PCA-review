\section*{\sffamily \Large INTRODUCTION}

Principal component Analysis (PCA) is one of the oldest, yet most widely used methods of unsupervised multivariate analysis. Given a $p$-dimensional random variable $\BX$ with mean vector ${\bf 0}_p$ and covariance matrix $\bfSigma$, the principal component transformation is defined as:
%
\begin{align}
\BX \mapsto \BY = \bfGamma^T \BX
\end{align}
%
where $\bfGamma$ is orthogonal, and $\bfGamma^T \bfSigma \bfGamma = \bfLambda = \diag( \lambda_1, \ldots, \lambda_p); \lambda_1 \geq \ldots \geq \lambda_p \geq 0$. This induces a set of linear transformations on the random vector $\bfX$ so that the transformations are uncorrelated with each other, and their variances ordered from highest to smallest \citep{MardiaKentBibby}. Given that $\bfSigma$ is positive definite, these transformations are given by the eigenvectors of $\bfSigma$ following its spectral decomposition. For a size-$n$ sample from the distribution of $\BX$, say $\bfX = (\bfx_1, \ldots, \bfx_n)^T$, the first principal component gives the linear combination of columns of $\bfX$ which maximizes sample variance:
%
\begin{align}\label{eqn:eqnPCA1}
\bfw_1 &= \argmax_{\| \bfw \| = 1} Var (\bfX \bfw ) = \argmax_{\| \bfw \| = 1} \bfw^T \bfX^T \bfX \bfw
\end{align}
%
and the subsequent principal components are defined as
\begin{align}\label{eqn:eqnPCAk}
\bfw_k &= \argmax_{\| \bfw \| = 1} \bfw^T \bfR_k^T \bfR_k \bfw; \quad \bfR_k = \bfX - \sum_{s=1}^{k-1} \bfX \bfw_s \bfw_s^T \quad \text{for } 1< k \leq p
\end{align}
%
Following a lagrange multiplier approach, the eigenvectors of $\bfX^T \bfX/n$, equivalently the right singular vectors obtained from the singular value decomposition of $\bfX$ provide solutions to (\ref{eqn:eqnPCA1}) and (\ref{eqn:eqnPCAk}).

PCA has seen extensive applications in diverse areas, such as image recognition \citep{AlkandariAljaber15}, finance \citep{AlexanderBook}, climate science \citep{WilksBook} and text mining \citep{BerryCastellanos}, with the objective being reduction of the intrinsic dimensionality of the feature space without losing information. However, because the objective function to be maximized is quadratic, PCA performs poorly in presence of even a small proportion of corrupted observations \citep{XuCaramanisMannor13}. Based on the domain of application, these corruptions can be the result of data heterogeneity \citep{SahaEtal16}, measurement error \citep{Bailey12,HelltonThoresen14}, or may represent structured noise \citep{CandesEtal09}.

Depending on the modelling goals, robust PCA aims to estimate principal components or the underlying low dimensional subspace in presence of corrupted entries in the data matrix. Historically, the instrumental factors behind the evolution of robust PCA methods have been the size and complexity of datasets, availability of computational resources, as well as the nature of corruptions present in the data. Early methods of robust PCA were focused on robustly estimating the population covariance matrix from datasets that are small to moderate in size, and comprised of independent samples: some of which were outliers, i.e. contained corrupted entries. Later on, the computational and statistical challenges that surfaced with the advent of high-dimensional datasets having a large number of features were tackled by methods like projection pursuit \citep{LiChen85}, ROBPCA \citep{hubert05} and M-estimation \citep{LocantoreEtal99,Majumdar15}.

This article is organized as follows. The theoretical discussion is composed of two sections, and we discuss the above broad approaches of robust PCA on independent data in further detail in the first of those sections. We devote the other section to Principal Component Pursuit (PCP), which, even though introduced very recently \citep{CandesEtal09}, has motivated a substantial amount of research on the problem of recovering an underlying low-rank structure in the data, rather than the principal components \textit{per se}, in presence of noise. We also illustrate the relevance and relative performance of these two types of methods using two real data examples. Following this we review the methods of robust PCA in domains that are not multivariate real, for example reproducing kernel hilbert spaces or the space of square-integrable functions, in the section {\it Robust PCA in Other Spaces}. We finally finish the review with section {\it Conclusion}. {\colrbf word it better}
